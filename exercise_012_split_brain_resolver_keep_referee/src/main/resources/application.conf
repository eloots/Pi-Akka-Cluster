akka {

  loggers          = [akka.event.slf4j.Slf4jLogger]
  loglevel         = debug
  log-dead-letters = on
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  logger-startup-timeout = 30s

  coordinated-shutdown.exit-jvm = on

  actor {

    provider = cluster

    debug {

      lifecycle = on
      unhandled = on
    }
  }

  cluster {

    seed-nodes = ["akka://pi-"${cluster-node-configuration.cluster-id}"-system@"${cluster-node-configuration.seed-node-1}":2550",
                  "akka://pi-"${cluster-node-configuration.cluster-id}"-system@"${cluster-node-configuration.seed-node-2}":2550",
                  "akka://pi-"${cluster-node-configuration.cluster-id}"-system@"${cluster-node-configuration.seed-node-3}":2550",
                  "akka://pi-"${cluster-node-configuration.cluster-id}"-system@"${cluster-node-configuration.seed-node-4}":2550"]

    downing-provider-class = "com.lightbend.akka.sbr.SplitBrainResolverProvider"

    split-brain-resolver {
      # Select one of the available strategies (see descriptions below):
      # static-quorum, keep-majority, keep-oldest, keep-referee
      # if left "off" when the downing provider is enabled cluster startup will fail.
      active-strategy = keep-referee

      keep-referee {
        address = "akka://pi-"${cluster-node-configuration.cluster-id}"-system@node-3:2550"
//        down-all-if-less-than-nodes = 3
      }

      # Time margin after which shards or singletons that belonged to a downed/removed
      # partition are created in surviving partition. The purpose of this margin is that
      # in case of a network partition the persistent actors in the non-surviving partitions
      # must be stopped before corresponding persistent actors are started somewhere else.
      # This is useful if you implement downing strategies that handle network partitions,
      # e.g. by keeping the larger side of the partition and shutting down the smaller side.
      # Decision is taken by the strategy when there has been no membership or
      # reachability changes for this duration, i.e. the cluster state is stable.
      stable-after = 7s
    }
  }

  remote {

    artery {
      transport = tcp

      enabled = on
 
      canonical {
        hostname = "127.0.0.1"
        port = 2550
      }
    }
  }
}

cinnamon {

//  akka.cluster {
//    domain-events = on
//    member-events = on
//    singleton-events = on
//    shard-region-info = on
//  }

  akka.actors {
    "/user/*" {
      report-by = instance
    }
  }

  prometheus {
    exporters += http-server

    http-server {
      host = "0.0.0.0"
    }
  }
}
