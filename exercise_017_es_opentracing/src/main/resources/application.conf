akka {

   discovery.method = akka-dns

   management.http {
     //host = 127.0.0.1
     port = 8558
   }

  extensions = ["akka.cluster.client.ClusterClientReceptionist"]

  loggers          = [akka.event.slf4j.Slf4jLogger]
  loglevel         = debug
  log-dead-letters = on
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  logger-startup-timeout = 30s

  coordinated-shutdown.exit-jvm = on

  actor {

    provider = cluster

    debug {

      lifecycle = on
      unhandled = on
    }
  }

  cluster {

    seed-node-timeout = 12 seconds

    seed-nodes = ["akka://pi-"${cluster-node-configuration.cluster-id}"-system@"${cluster-node-configuration.seed-node-1}":2550",
                  "akka://pi-"${cluster-node-configuration.cluster-id}"-system@"${cluster-node-configuration.seed-node-2}":2550",
                  "akka://pi-"${cluster-node-configuration.cluster-id}"-system@"${cluster-node-configuration.seed-node-3}":2550",
                  "akka://pi-"${cluster-node-configuration.cluster-id}"-system@"${cluster-node-configuration.seed-node-4}":2550"]

    downing-provider-class = "com.lightbend.akka.sbr.SplitBrainResolverProvider"

    split-brain-resolver {
      # Select one of the available strategies (see descriptions below):
      # static-quorum, keep-majority, keep-oldest, keep-referee
      # if left "off" when the downing provider is enabled cluster startup will fail.
      active-strategy = static-quorum

      static-quorum {
        quorum-size = 3
      }

      # Time margin after which shards or singletons that belonged to a downed/removed
      # partition are created in surviving partition. The purpose of this margin is that
      # in case of a network partition the persistent actors in the non-surviving partitions
      # must be stopped before corresponding persistent actors are started somewhere else.
      # This is useful if you implement downing strategies that handle network partitions,
      # e.g. by keeping the larger side of the partition and shutting down the smaller side.
      # Decision is taken by the strategy when there has been no membership or
      # reachability changes for this duration, i.e. the cluster state is stable.
      stable-after = 7s
    }
  }

  remote {

    artery {
      transport = tcp

      enabled = on
 
      canonical {
        hostname = "127.0.0.1"
        port = 2550
      }
    }
  }
}

cinnamon {
  akka.cluster {
    domain-events = on
    member-events = on
    node-events = on
    singleton-events = on
    shard-region-info = on
  }

  akka.actors {
    "/user/*" {
      report-by = instance
      traceable = on
      excludes = ["akka.http.*", "akka.stream.*"]
    }
  }

  akka.http {
    servers {
      "*:*" {
        paths {
          "*" {
            metrics = on
            traceable = on
          }
        }
      }
    }

    clients {
      "*:*" {
        paths {
          "*" {
            metrics = on
            traceable = on
          }
        }
      }
    }
  }

  opentracing {
    # Const sampler trace only for demo purposes -> never in production!
    tracer {
      sampler = const-sampler
      const-sampler {
        decision = true
      }
    }

    akka.trace-system-messages = yes

    zipkin {
      url-connection {
        # FIXME : THE IP NEEDS TO POINT TO THE MACHINE WHERE ZIPKIN IS RUNNING
        endpoint = "http://192.168.0.28:9411/api/v1/spans"
      }
    }
  }

  prometheus {
    exporters += http-server
    http-server {
      host = "0.0.0.0"
    }
  }
}
